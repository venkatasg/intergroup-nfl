{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e70c37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x104A83340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import re\n",
    "import csv\n",
    "from evaluate import load\n",
    "\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from sklearn.metrics import precision_score as prec, recall_score as recall, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplcursors\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='darkgrid', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=2)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "# Set random seeds for reproducibility on a specific machine\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d7ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../outputs/ref-labels.txt', 'r') as f:\n",
    "    label_sents = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42427b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = pd.read_csv('../data/test_data.tsv', sep='\\t', quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")\n",
    "test = gold[gold.split=='test']\n",
    "test.ref_expressions = test.ref_expressions.apply(lambda x: eval(x))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91534afe",
   "metadata": {},
   "source": [
    "## Distribution of labels in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c29b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IN]:  303\n",
      "[OUT]:  63\n",
      "[OTHER]:  46\n",
      "No tag comments : 77\n"
     ]
    }
   ],
   "source": [
    "test_set = \" \".join(label_sents)\n",
    "\n",
    "print(\"[IN]: \", test_set.count(\"[IN]\"))\n",
    "print(\"[OUT]: \", test_set.count(\"[OUT]\"))\n",
    "print(\"[OTHER]: \", test_set.count(\"[OTHER]\"))\n",
    "print(\"No tag comments :\", len([x for x in label_sents if not(('[IN]' in x) or ('[OUT]' in x) or ('[OTHER]' in x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ed07bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[IN]': 303, '[OUT]': 63, '[OTHER]': 46, 'None': 77}, 412)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {'[IN]': ' '.join(label_sents).count('[IN]'), \n",
    "          '[OUT]': ' '.join(label_sents).count('[OUT]'), \n",
    "          '[OTHER]': ' '.join(label_sents).count('[OTHER]'), \n",
    "          'None': len([x for x in label_sents if not(('[IN]' in x) or ('[OUT]' in x) or ('[OTHER]' in x))])}\n",
    "all_tags_count = counts['[IN]'] + counts['[OUT]'] + counts['[OTHER]']\n",
    "counts,all_tags_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc8880e-ed25-445a-8aed-e44e8b56ef4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[IN]': 0.735, '[OUT]': 0.153, '[OTHER]': 0.112}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_counts = {x: np.round(counts[x]/all_tags_count, 3) for x in ['[IN]', '[OUT]', '[OTHER]']}\n",
    "# weight_counts['None'] = np.round(counts['None']/len(label_sents), 3)\n",
    "weight_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f78e4c-a490-4bac-9751-6cfa5cb00aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of weighting F1s by their actual counts, give equal weights to all classes\n",
    "weight_equal = {'[IN]': 0.34, '[OUT]': 0.33, '[OTHER]': 0.33}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a034988",
   "metadata": {},
   "source": [
    "## Confusion matrix and scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb846ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conf_mat(pred_sents, label_sents):\n",
    "    conf_mat = {'[IN]': 0, '[OUT]': 0, '[OTHER]': 0, 'None': 0}\n",
    "    label_mat = {'[IN]': conf_mat.copy(), '[OUT]': conf_mat.copy(), '[OTHER]': conf_mat.copy(), 'None': conf_mat.copy()}\n",
    "\n",
    "    pattern = re.compile(r'(\\[IN\\]|\\[OUT\\]|\\[OTHER\\])')\n",
    "    for ind in range(len(label_sents)):\n",
    "        pred = pred_sents[ind]\n",
    "        ref = label_sents[ind]\n",
    "\n",
    "        if len(re.findall(pattern, ref))==0:\n",
    "            if len(re.findall(pattern, pred))==0:\n",
    "                label_mat['None']['None'] += 1\n",
    "            else:\n",
    "                if '[IN]' in pred:\n",
    "                    label_mat['None']['[IN]'] += 1\n",
    "                elif '[OUT]' in pred:\n",
    "                    label_mat['None']['[OUT]'] += 1\n",
    "                elif '[OTHER]' in pred:\n",
    "                    label_mat['None']['[OTHER]'] += 1\n",
    "\n",
    "        for match in pattern.finditer(ref):\n",
    "            if '[IN]' in pred[match.start()-1:match.end()+1]:\n",
    "                    label_mat[match.group()]['[IN]'] += 1\n",
    "            elif '[OUT]' in pred[match.start()-1:match.end()+1]:\n",
    "                label_mat[match.group()]['[OUT]'] += 1\n",
    "            elif '[OTHER]' in pred[match.start()-1:match.end()+1]:\n",
    "                label_mat[match.group()]['[OTHER]'] += 1\n",
    "            else:\n",
    "                if '[IN]' in pred[match.start()-3:match.end()+3]:\n",
    "                    label_mat[match.group()]['[IN]'] += 0.5\n",
    "                elif '[OUT]' in pred[match.start()-3:match.end()+3]:\n",
    "                    label_mat[match.group()]['[OUT]'] += 0.5\n",
    "                elif '[OTHER]' in pred[match.start()-3:match.end()+3]:\n",
    "                    label_mat[match.group()]['[OTHER]'] += 0.5\n",
    "                else:\n",
    "                    if '[IN]' in pred[match.start()-5:match.end()+5]:\n",
    "                        label_mat[match.group()]['[IN]'] += 0.25\n",
    "                    elif '[OUT]' in pred[match.start()-5:match.end()+5]:\n",
    "                        label_mat[match.group()]['[OUT]'] += 0.25\n",
    "                    elif '[OTHER]' in pred[match.start()-5:match.end()+5]:\n",
    "                        label_mat[match.group()]['[OTHER]'] += 0.25\n",
    "                    label_mat[match.group()]['None'] += 1\n",
    "    return label_mat\n",
    "\n",
    "def print_scores(label_mat, counts, all_tags_count):\n",
    "    recs = {}\n",
    "    precs = {}\n",
    "    f1s = {}\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    for tag in  ['[IN]', '[OUT]', '[OTHER]']:\n",
    "        tp = label_mat[tag][tag]\n",
    "        recs[tag] = np.round(tp/(counts[tag]), 3)\n",
    "        precs[tag] = np.round(tp/sum([label_mat[x][tag] for x in ['[IN]', '[OUT]', '[OTHER]', 'None']]), 3)\n",
    "        tp_count += tp\n",
    "        fp_count += sum([label_mat[tag][x] if x!=tag else 0 for x in ['[IN]', '[OUT]', '[OTHER]', 'None']])\n",
    "\n",
    "    recs['micro_avg'] = np.round(tp_count/all_tags_count, 3)\n",
    "    precs['micro_avg'] = np.round(tp_count/(tp_count+fp_count), 3)\n",
    "\n",
    "    for tag in  ['[IN]', '[OUT]', '[OTHER]', 'micro_avg']:\n",
    "        f1s[tag] = np.round(2*precs[tag]*recs[tag]/(precs[tag]+recs[tag]), 3)\n",
    "\n",
    "    f1_macro = np.round(np.mean([f1s[x] for x in ['[IN]', '[OUT]', '[OTHER]']]), 3)\n",
    "    f1_macro_weighted = np.round(sum([f1s[x]*weight_counts[x] for x in ['[IN]', '[OUT]', '[OTHER]']]), 3)\n",
    "    none_acc = np.round(label_mat['None']['None']/counts['None'], 3)\n",
    "        \n",
    "    table = pd.DataFrame([recs, precs, f1s], index=['recall', 'precision', 'f1'])\n",
    "    display(table, f\"Macro F1: {f1_macro}\", f\"Weighted F1: {f1_macro_weighted}\", f\"None accuracy: {none_acc}\")\n",
    "\n",
    "# wer = load('wer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61e05d-d00b-463a-be14-2d3a19ecab41",
   "metadata": {},
   "source": [
    "# Calculate recall, precision, F1 for model output of interest here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31f1b35-9067-4b82-9ad2-70b6f601399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[IN]</th>\n",
       "      <th>[OUT]</th>\n",
       "      <th>[OTHER]</th>\n",
       "      <th>micro_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.553</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.898</td>\n",
       "      <td>0.767</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.684</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            [IN]  [OUT]  [OTHER]  micro_avg\n",
       "recall     0.553  0.365    0.337      0.500\n",
       "precision  0.898  0.767    1.000      0.519\n",
       "f1         0.684  0.495    0.504      0.509"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1: 0.561'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Weighted F1: 0.635'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'None accuracy: 0.922'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../outputs/seed-1-sample-sents-chkpt.txt', 'r') as f:\n",
    "    pred_sents = [x.strip()  for x in f.readlines()]\n",
    "\n",
    "label_mat = build_conf_mat(pred_sents, label_sents)\n",
    "print_scores(label_mat, counts, all_tags_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6f6cbc21-2ef9-4940-896b-5bb306b634f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[IN]': {'[IN]': 185.25, '[OUT]': 4, '[OTHER]': 1.0, 'None': 96},\n",
       " '[OUT]': {'[IN]': 6.25, '[OUT]': 31.0, '[OTHER]': 1.5, 'None': 20},\n",
       " '[OTHER]': {'[IN]': 6.0, '[OUT]': 1, '[OTHER]': 17.25, 'None': 20},\n",
       " 'None': {'[IN]': 13, '[OUT]': 0, '[OTHER]': 2, 'None': 62}}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7835789f-508e-4806-a7bd-f79fe29491cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[IN]': {'[IN]': 169.5, '[OUT]': 2.25, '[OTHER]': 1.0, 'None': 124},\n",
       " '[OUT]': {'[IN]': 3, '[OUT]': 33.75, '[OTHER]': 0, 'None': 25},\n",
       " '[OTHER]': {'[IN]': 1, '[OUT]': 2.5, '[OTHER]': 14.0, 'None': 27},\n",
       " 'None': {'[IN]': 9, '[OUT]': 1, '[OTHER]': 0, 'None': 67}}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa64c9b-d71b-41b3-8827-96e823a9cc8e",
   "metadata": {},
   "source": [
    "### Chance F1 scores? With and without uniform prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1872d939-548e-4722-b448-fbc92556914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[IN]</th>\n",
       "      <th>[OUT]</th>\n",
       "      <th>[OTHER]</th>\n",
       "      <th>micro_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.228</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.633</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.335</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            [IN]  [OUT]  [OTHER]  micro_avg\n",
       "recall     0.228  0.302    0.152      0.231\n",
       "precision  0.633  0.162    0.051      0.231\n",
       "f1         0.335  0.211    0.076      0.231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1: 0.207'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Weighted F1: 0.287'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'None accuracy: 0.247'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "\n",
    "conf_mat = {'[IN]': 0, '[OUT]': 0, '[OTHER]': 0, 'None': 0}\n",
    "\n",
    "label_mat = {'[IN]': conf_mat.copy(), '[OUT]': conf_mat.copy(), '[OTHER]': conf_mat.copy(), 'None': conf_mat.copy()}\n",
    "\n",
    "pattern = re.compile(r'(\\[IN\\]|\\[OUT\\]|\\[OTHER\\])')\n",
    "\n",
    "for ind in range(len(label_sents)):\n",
    "    ref = label_sents[ind]\n",
    "    if len(re.findall(pattern, ref))==0:\n",
    "        elem = random.sample(['[IN]', '[OUT]', '[OTHER]', 'None'], counts=[1,1,1,1], k=1)\n",
    "        label_mat['None'][elem[0]] += 1\n",
    "    for match in pattern.finditer(ref):\n",
    "        elem = random.sample(['[IN]', '[OUT]', '[OTHER]', 'None'], counts=[1,1,1,1], k=1)\n",
    "        label_mat[match.group()][elem[0]] += 1\n",
    "\n",
    "print_scores(label_mat, counts, all_tags_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78e78354-545b-4dc8-95b2-12780f429ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[IN]</th>\n",
       "      <th>[OUT]</th>\n",
       "      <th>[OTHER]</th>\n",
       "      <th>micro_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.618</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            [IN]  [OUT]  [OTHER]  micro_avg\n",
       "recall     0.620  0.111    0.043      0.478\n",
       "precision  0.616  0.143    0.041      0.478\n",
       "f1         0.618  0.125    0.042      0.478"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Macro F1: 0.262'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Weighted F1: 0.478'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'None accuracy: 0.143'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "conf_mat = {'[IN]': 0, '[OUT]': 0, '[OTHER]': 0, 'None': 0}\n",
    "\n",
    "label_mat = {'[IN]': conf_mat.copy(), '[OUT]': conf_mat.copy(), '[OTHER]': conf_mat.copy(), 'None': conf_mat.copy()}\n",
    "\n",
    "pattern = re.compile(r'(\\[IN\\]|\\[OUT\\]|\\[OTHER\\])')\n",
    "\n",
    "for ind in range(len(label_sents)):\n",
    "    ref = label_sents[ind]\n",
    "    if len(re.findall(pattern, ref))==0:\n",
    "        elem = random.sample(['[IN]', '[OUT]', '[OTHER]', 'None'], counts=[counts[x] for x in ['[IN]', '[OUT]', '[OTHER]', 'None']], k=1)\n",
    "        label_mat['None'][elem[0]] += 1\n",
    "    for match in pattern.finditer(ref):\n",
    "        elem = random.sample(['[IN]', '[OUT]', '[OTHER]', 'None'], counts=[counts[x] for x in ['[IN]', '[OUT]', '[OTHER]', 'None']], k=1)\n",
    "        label_mat[match.group()][elem[0]] += 1\n",
    "\n",
    "print_scores(label_mat, counts, all_tags_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b551a-e675-42f4-b816-a2f7fc9f19c1",
   "metadata": {},
   "source": [
    "## Bootstrap testing different conditions for significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faedd5f-0863-4df3-a211-8fdab93f4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_b_over_a = 0\n",
    "ling_wp = [58.4, 60.1, 60.3]\n",
    "ling_wp_temp = [60.4, 60.1, 60.6]\n",
    "wp_temp = [57.1, 59.5, 56.2]\n",
    "wp = [56, 58, 55.8]\n",
    "no_wp = [55.9, 56.3, 58.8]\n",
    "no_wp_temp = [57.5, 59.5, 60.8]\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def bootstrap(x, y, num_samples=100000):\n",
    "    num_b_greater_a = 0\n",
    "    for i in range(num_samples):\n",
    "        a = random.choice(x)\n",
    "        b = random.choice(y)\n",
    "        if b-a>0.00001:\n",
    "            num_b_greater_a += 1\n",
    "    p_sig = 1-np.round((num_b_greater_a/num_samples), 3)\n",
    "    if p_sig<0.05:\n",
    "        print(\"True\")\n",
    "\n",
    "bootstrap(no_wp, ling_wp_temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
