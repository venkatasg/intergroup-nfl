{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf12cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 20:01:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ef138456824dea91533be7c0732713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 20:01:12 INFO: Downloaded file to /Users/venkat/stanza_resources/resources.json\n",
      "2024-06-27 20:01:12 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-27 20:01:12 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2024-06-27 20:01:12 INFO: Using device: cpu\n",
      "2024-06-27 20:01:12 INFO: Loading: tokenize\n",
      "2024-06-27 20:01:12 INFO: Loading: mwt\n",
      "2024-06-27 20:01:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import re\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "import csv\n",
    "import stanza\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='darkgrid', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=2)\n",
    "from more_itertools import sliced\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "# Set random seeds for reproducibility on a specific machine\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(1)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408f217b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14421566, 33)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for file in glob('../data/raw_data/*.tsv'):\n",
    "    dfs.append(\n",
    "        pd.read_csv(\n",
    "            file, \n",
    "            sep='\\t', \n",
    "            index_col=None, \\\n",
    "            header=0, \n",
    "            low_memory=False,\n",
    "            quotechar='\"',\n",
    "#             dtype = {\n",
    "#                 'post_id': str,\n",
    "#                 'comment_id': str,\n",
    "#                 'parent_id': str,\n",
    "#                 'raw_comment': str,\n",
    "#                 'timestamp': float,\n",
    "#                 'subreddit': str,\n",
    "#                 'username': str,\n",
    "#                 'flair': str,\n",
    "#                 'score': float\n",
    "#             },\n",
    "#             on_bad_lines='skip'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df.shape[0], len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949d41ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>username</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>clean_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_10or15q</td>\n",
       "      <td>j6gcw7b</td>\n",
       "      <td>t3_10or15q</td>\n",
       "      <td>Burrow got pressured alot. Didn't like that.</td>\n",
       "      <td>1675048122</td>\n",
       "      <td>bengals</td>\n",
       "      <td>mccurdy3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>476.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_10or15q</td>\n",
       "      <td>j6gd9xk</td>\n",
       "      <td>t1_j6gcw7b</td>\n",
       "      <td>Yeah, when your second string guard is going up against a DPOY candidate. It's not gonna end well.</td>\n",
       "      <td>1675048261</td>\n",
       "      <td>bengals</td>\n",
       "      <td>mxyztplk33</td>\n",
       "      <td></td>\n",
       "      <td>114.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id comment_id   parent_id  \\\n",
       "0  t3_10or15q    j6gcw7b  t3_10or15q   \n",
       "1  t3_10or15q    j6gd9xk  t1_j6gcw7b   \n",
       "\n",
       "                                                                                          raw_comment  \\\n",
       "0                                                        Burrow got pressured alot. Didn't like that.   \n",
       "1  Yeah, when your second string guard is going up against a DPOY candidate. It's not gonna end well.   \n",
       "\n",
       "    timestamp subreddit    username flair  score clean_comment  \n",
       "0  1675048122   bengals    mccurdy3   NaN  476.0           NaN  \n",
       "1  1675048261   bengals  mxyztplk33        114.0           NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dfs\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4da73",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf66570b-22b6-46e2-929c-ef6cff33cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp must be float\n",
    "df.timestamp = pd.to_numeric(df.timestamp,errors='coerce')\n",
    "df.score = pd.to_numeric(df.score,errors='coerce')\n",
    "\n",
    "# Drop all rows without post_id, comment_id, raw_comment, timestamp or subreddit\n",
    "df = df.dropna(subset=['post_id', 'comment_id', 'raw_comment', 'timestamp', 'subreddit'])\n",
    "\n",
    "# Remove newlines in string columns and deleted comments\n",
    "df['clean_comment'] = df.raw_comment.apply(lambda x : x.replace('\\n', ' '))\n",
    "df.flair = df.flair.apply(lambda x : str(x).replace('\\n', ' '))\n",
    "df['clean_comment'] = df['clean_comment'].apply(lambda x : x.replace('\\t', ' '))\n",
    "df.flair = df.flair.apply(lambda x : str(x).replace('\\t', ' '))\n",
    "df['clean_comment'] = df['clean_comment'].apply(lambda x : x.replace('\\r', ' '))\n",
    "df.flair = df.flair.apply(lambda x : str(x).replace('\\r', ' '))\n",
    "df = df[df['clean_comment']!=\"[deleted]\"]\n",
    "df = df[df['clean_comment']!=\"[removed]\"]\n",
    "\n",
    "# Remove Reddit formatting for URLS and replace with just link text\n",
    "df['clean_comment'] = df['clean_comment'].apply(lambda x: re.sub(r'\\[(.*)\\]\\((.*)\\)', '\\g<1>', x, flags=re.IGNORECASE))\n",
    "\n",
    "# Remove explicit URLs\n",
    "df['clean_comment'] = df['clean_comment'].apply(lambda x: re.sub(r'http[s]*\\S+', 'URL', x, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846ec551-c4e0-4944-a032-da5be66568aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove prefix for posts and only select post items\n",
    "df = df[df['post_id'].str.startswith('t3_')]\n",
    "df['post_id'] = df['post_id'].apply(lambda x: x[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12152de-ba58-4cb2-8ea8-fc1f9567ef22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>username</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>clean_comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10or15q</th>\n",
       "      <th>j6gcw7b</th>\n",
       "      <td>10or15q</td>\n",
       "      <td>j6gcw7b</td>\n",
       "      <td>t3_10or15q</td>\n",
       "      <td>Burrow got pressured alot. Didn't like that.</td>\n",
       "      <td>1.675048e+09</td>\n",
       "      <td>bengals</td>\n",
       "      <td>mccurdy3</td>\n",
       "      <td>nan</td>\n",
       "      <td>476.0</td>\n",
       "      <td>Burrow got pressured alot. Didn't like that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j6gd9xk</th>\n",
       "      <td>10or15q</td>\n",
       "      <td>j6gd9xk</td>\n",
       "      <td>t1_j6gcw7b</td>\n",
       "      <td>Yeah, when your second string guard is going up against a DPOY candidate. It's not gonna end well.</td>\n",
       "      <td>1.675048e+09</td>\n",
       "      <td>bengals</td>\n",
       "      <td>mxyztplk33</td>\n",
       "      <td></td>\n",
       "      <td>114.0</td>\n",
       "      <td>Yeah, when your second string guard is going up against a DPOY candidate. It's not gonna end well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j6gd57u</th>\n",
       "      <td>10or15q</td>\n",
       "      <td>j6gd57u</td>\n",
       "      <td>t1_j6gcw7b</td>\n",
       "      <td>3 backups caught up to us. We had a healthy line when we mauled them week 13.</td>\n",
       "      <td>1.675048e+09</td>\n",
       "      <td>bengals</td>\n",
       "      <td>USAesNumeroUno</td>\n",
       "      <td></td>\n",
       "      <td>186.0</td>\n",
       "      <td>3 backups caught up to us. We had a healthy line when we mauled them week 13.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j6gjd30</th>\n",
       "      <td>10or15q</td>\n",
       "      <td>j6gjd30</td>\n",
       "      <td>t1_j6gd57u</td>\n",
       "      <td>We never mauled them. This comment is just as worse as the “burrowhead” and whatever tf that cincy mayor was on</td>\n",
       "      <td>1.675051e+09</td>\n",
       "      <td>bengals</td>\n",
       "      <td>Lowered_expectationz</td>\n",
       "      <td>nan</td>\n",
       "      <td>17.0</td>\n",
       "      <td>We never mauled them. This comment is just as worse as the “burrowhead” and whatever tf that cincy mayor was on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j6hcxd0</th>\n",
       "      <td>10or15q</td>\n",
       "      <td>j6hcxd0</td>\n",
       "      <td>t1_j6gd57u</td>\n",
       "      <td>Chiefs dealt with the same problem in their SB loss to Tampa Bay. Definitely no shame in a 3 point loss on the road with a makeshift OL. Bengals have Burrow and the best WR corps in the NFL on cheap contracts still, along with great coaching. They're in a great position to make another deep run next year, and not just a hollow \"they'll be back,\" like some pretenders of the past.</td>\n",
       "      <td>1.675071e+09</td>\n",
       "      <td>bengals</td>\n",
       "      <td>Last_Account_Ever</td>\n",
       "      <td></td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chiefs dealt with the same problem in their SB loss to Tampa Bay. Definitely no shame in a 3 point loss on the road with a makeshift OL. Bengals have Burrow and the best WR corps in the NFL on cheap contracts still, along with great coaching. They're in a great position to make another deep run next year, and not just a hollow \"they'll be back,\" like some pretenders of the past.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    post_id comment_id   parent_id  \\\n",
       "post_id comment_id                                   \n",
       "10or15q j6gcw7b     10or15q    j6gcw7b  t3_10or15q   \n",
       "        j6gd9xk     10or15q    j6gd9xk  t1_j6gcw7b   \n",
       "        j6gd57u     10or15q    j6gd57u  t1_j6gcw7b   \n",
       "        j6gjd30     10or15q    j6gjd30  t1_j6gd57u   \n",
       "        j6hcxd0     10or15q    j6hcxd0  t1_j6gd57u   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                      raw_comment  \\\n",
       "post_id comment_id                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "10or15q j6gcw7b                                                                                                                                                                                                                                                                                                                                                      Burrow got pressured alot. Didn't like that.   \n",
       "        j6gd9xk                                                                                                                                                                                                                                                                                                Yeah, when your second string guard is going up against a DPOY candidate. It's not gonna end well.   \n",
       "        j6gd57u                                                                                                                                                                                                                                                                                                                     3 backups caught up to us. We had a healthy line when we mauled them week 13.   \n",
       "        j6gjd30                                                                                                                                                                                                                                                                                   We never mauled them. This comment is just as worse as the “burrowhead” and whatever tf that cincy mayor was on   \n",
       "        j6hcxd0     Chiefs dealt with the same problem in their SB loss to Tampa Bay. Definitely no shame in a 3 point loss on the road with a makeshift OL. Bengals have Burrow and the best WR corps in the NFL on cheap contracts still, along with great coaching. They're in a great position to make another deep run next year, and not just a hollow \"they'll be back,\" like some pretenders of the past.   \n",
       "\n",
       "                       timestamp subreddit              username flair  score  \\\n",
       "post_id comment_id                                                              \n",
       "10or15q j6gcw7b     1.675048e+09   bengals              mccurdy3   nan  476.0   \n",
       "        j6gd9xk     1.675048e+09   bengals            mxyztplk33        114.0   \n",
       "        j6gd57u     1.675048e+09   bengals        USAesNumeroUno        186.0   \n",
       "        j6gjd30     1.675051e+09   bengals  Lowered_expectationz   nan   17.0   \n",
       "        j6hcxd0     1.675071e+09   bengals     Last_Account_Ever          5.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                    clean_comment  \n",
       "post_id comment_id                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "10or15q j6gcw7b                                                                                                                                                                                                                                                                                                                                                      Burrow got pressured alot. Didn't like that.  \n",
       "        j6gd9xk                                                                                                                                                                                                                                                                                                Yeah, when your second string guard is going up against a DPOY candidate. It's not gonna end well.  \n",
       "        j6gd57u                                                                                                                                                                                                                                                                                                                     3 backups caught up to us. We had a healthy line when we mauled them week 13.  \n",
       "        j6gjd30                                                                                                                                                                                                                                                                                   We never mauled them. This comment is just as worse as the “burrowhead” and whatever tf that cincy mayor was on  \n",
       "        j6hcxd0     Chiefs dealt with the same problem in their SB loss to Tampa Bay. Definitely no shame in a 3 point loss on the road with a makeshift OL. Bengals have Burrow and the best WR corps in the NFL on cheap contracts still, along with great coaching. They're in a great position to make another deep run next year, and not just a hollow \"they'll be back,\" like some pretenders of the past.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index(['post_id', 'comment_id'], drop=False, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7169d583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def tokenize(comments):\n",
    "#     docs = nlp.stream('\\n\\n'.join(comments))\n",
    "#     new_comments = []\n",
    "#     for doc in docs:\n",
    "#         ipdb.set_trace()\n",
    "#         new_comment = ''\n",
    "#         for sent in doc.sentences:\n",
    "#             new_comment += '[CLS] ' + ' '.join([token.text for token in sent.tokens]) + ' ' \n",
    "#         new_comments.append(new_comment)\n",
    "#     assert len(new_comments)==len(comments)\n",
    "#     return new_comments\n",
    "\n",
    "# tokenized_comments = []\n",
    "# # Tokenize and add cls tokens\n",
    "# for chunk in tqdm(sliced(df, 100), total=len(df)//100):\n",
    "#     comments= chunk['clean_comment'].values.tolist()\n",
    "#     tokenized_comments.append(tokenize(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88144622",
   "metadata": {},
   "outputs": [],
   "source": [
    "gameinfo = pd.read_csv('../data/gameInfo.tsv', sep='\\t')\n",
    "postinfo = pd.read_csv('../data/postInfo.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67822afc",
   "metadata": {},
   "source": [
    "### Restrict comments to those where we got post info, and thus game info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838f081f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458192, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['post_id'].isin(postinfo['post_id'].unique())]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04fd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaminfo = pd.read_csv('../data/nfl_teams.csv')\n",
    "teaminfo['team_name_short'] = teaminfo['team_name_short'].apply(lambda x: x.lower())\n",
    "\n",
    "teams = teaminfo['team_name_short'].values.tolist()\n",
    "subreddits = teaminfo['subreddit'].values.tolist()\n",
    "\n",
    "teams_to_subreddit = {teams[i]: subreddits[i] for i in range(32)}\n",
    "subreddit_to_teams = {subreddits[i]: teams[i] for i in range(32)}\n",
    "team_names_dict = {x: [x] for x in teams}\n",
    "\n",
    "for x in teams:\n",
    "    team_names_dict[x].append(teaminfo[teaminfo['team_name_short']==x]['team_id'].values[0])\n",
    "    team_names_dict[x].append(teaminfo[teaminfo['team_name_short']==x]['team_id_pfr'].values[0])\n",
    "    # get approx location name? There will be overlaps\n",
    "    city_name = teaminfo[teaminfo['team_name_short']==x]['team_name'].values[0].lower().replace(x,'').strip()\n",
    "    \n",
    "    team_names_dict[x].append(city_name)\n",
    "    \n",
    "    team_names_dict[x] = list(set(team_names_dict[x]))\n",
    "\n",
    "# A fix for washington commanders error prone data\n",
    "team_names_dict['commanders'].append('redskins')\n",
    "team_names_dict['washington'] = team_names_dict['commanders']\n",
    "\n",
    "team_names_dict['buccaneers'].append('bucs')\n",
    "team_names_dict['jaguars'].append('jags')\n",
    "team_names_dict['patriots'].append('pats')\n",
    "team_names_dict['eagles'].append('philly')\n",
    "team_names_dict['colts'].append('indiana')\n",
    "team_names_dict['colts'].append('dolts')\n",
    "team_names_dict['dolphins'].append('phins')\n",
    "team_names_dict['chargers'].append('bolts')\n",
    "\n",
    "# df['team'] = df['subreddit'].apply(lambda x: subreddit_to_team(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec14e35",
   "metadata": {},
   "source": [
    "Figure out the opponent team and cache it in an array for now. Do it per thread to save time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d57f6-947e-4f15-bcb8-44099f8cd268",
   "metadata": {},
   "source": [
    "## Add gametime and win prob and opp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da889fc-b00d-4a93-9740-017a7abd5087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458192, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "142f3dcf-29a5-40f7-aff6-d58530868a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp = pd.read_csv('play_by_play.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "def get_utc_timestamp(row):\n",
    "    tod = row['time_of_day']\n",
    "    date = row['game_date']\n",
    "    \n",
    "    day = int(date.split('-')[2])\n",
    "    month = int(date.split('-')[1])\n",
    "    year = int(date.split('-')[0])\n",
    "    \n",
    "    hour = int(tod.split(':')[0])\n",
    "    minute = int(tod.split(':')[1])\n",
    "    second = int(tod.split(':')[2])\n",
    "    \n",
    "    # UTC time for NFL games is almost always afternoon? The earliest local time is around 11am, which would be at least 3pm in UTC\n",
    "    if int(hour)<12: \n",
    "        day_obj = dt.date(year, month, day)\n",
    "        day = (day_obj + dt.timedelta(days=1)).day\n",
    "    \n",
    "    timestamp = dt.datetime(year, month, day, hour, minute, second, tzinfo=dt.timezone.utc)\n",
    "    return int(timestamp.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35163ac9-2b3f-491a-af38-d2fec763614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_prob(row, game_pbp, is_away_team):\n",
    "    wp = 0\n",
    "    if row.gametime <= 0.0:\n",
    "        wp = game_pbp.vegas_home_wp.values[0]\n",
    "    elif row.gametime >= 1.0:\n",
    "        wp = game_pbp.vegas_home_wp.values[-1]\n",
    "    else:\n",
    "        wp = game_pbp[game_pbp.timestamp<row.timestamp].tail(1).to_dict(orient='records')[0]['vegas_home_wp']\n",
    "    if is_away_team:\n",
    "        wp = 1 - wp\n",
    "    return np.round(wp, 3)\n",
    "\n",
    "def get_gametime(row, game_pbp):\n",
    "    game_start = game_pbp.timestamp.min()\n",
    "    game_end = game_pbp.timestamp.max()\n",
    "    if row.timestamp<=game_start:\n",
    "        return 0.0\n",
    "    elif row.timestamp >= game_end:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return np.round((row.timestamp-game_start)/(game_end-game_start),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3646cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 2405/2405 [1:22:49<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "df['opp'] = \"\"\n",
    "df['gametime'] = 0.0\n",
    "df['win_prob'] = 0.0\n",
    "\n",
    "for pid in tqdm(df.post_id.unique()):\n",
    "\n",
    "    # Get PBP for that game\n",
    "    gameid = postinfo.loc[postinfo.post_id==pid, 'game_id'].values[0]\n",
    "    game_pbp = pbp.loc[pbp['new_game_id']==gameid, ['game_date','time_of_day','posteam', 'home_team', 'away_team', 'desc', 'vegas_home_wp']].reset_index(drop=True)\n",
    "    game_pbp = game_pbp.bfill().ffill()\n",
    "    game_pbp['timestamp'] = game_pbp.apply(lambda x: get_utc_timestamp(x), axis=1)\n",
    "    game_pbp.sort_values('timestamp', ignore_index=True, inplace=True)\n",
    "    \n",
    "    in_team = subreddit_to_teams[df[df['post_id']==pid]['subreddit'].values[0]]\n",
    "    home_team_name = teaminfo.loc[teaminfo['team_id']==game_pbp['home_team'].values[0], 'team_name_short'].values[0]\n",
    "    \n",
    "    # Set opponent column\n",
    "    if home_team_name!=in_team:\n",
    "        opp = game_pbp['away_team'].values[0]\n",
    "    else:\n",
    "        opp = game_pbp['home_team'].values[0]\n",
    "    df.loc[df['post_id']==pid, 'opp'] = teaminfo.loc[teaminfo['team_id']==opp, 'team_name_short'].values[0]\n",
    "    \n",
    "    # Pre-game threads\n",
    "    if postinfo[postinfo['post_id']==pid]['type'].values[0]=='pre':\n",
    "        df.loc[df['post_id']==pid, 'gametime'] =  0.0\n",
    "        if in_team==home_team_name:\n",
    "            df.loc[df['post_id']==pid, 'win_prob'] =  np.round(game_pbp.vegas_home_wp.values[0], 3)\n",
    "        else:\n",
    "            df.loc[df['post_id']==pid, 'win_prob'] =  np.round(1-game_pbp.vegas_home_wp.values[0],3)\n",
    "    \n",
    "    # Post-game threads\n",
    "    elif postinfo.loc[postinfo['post_id']==pid]['type'].values[0]=='post':\n",
    "        df.loc[df['post_id']==pid, 'gametime'] =  1.0\n",
    "        if in_team==home_team_name:\n",
    "            df.loc[df['post_id']==pid, 'win_prob'] =  np.round(game_pbp.vegas_home_wp.values[-1], 3)\n",
    "        else:\n",
    "            df.loc[df['post_id']==pid, 'win_prob'] =  np.round(1-game_pbp.vegas_home_wp.values[-1],3)\n",
    "    \n",
    "    # Game threads\n",
    "    else:\n",
    "        new_df = df.loc[df['post_id']==pid]\n",
    "        new_df['gametime'] = new_df.apply(lambda x: get_gametime(x, game_pbp), axis=1)\n",
    "        df.loc[df['post_id']==pid, 'gametime'] = new_df['gametime']\n",
    "        df.loc[df['post_id']==pid, 'win_prob'] = new_df.apply(lambda x: get_win_prob(x, game_pbp, in_team==home_team_name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507ce18-b6a2-479f-bf6c-8a1fe607fc2a",
   "metadata": {},
   "source": [
    "## Write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d7e90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_types = dict(zip(postinfo['post_id'], postinfo['type']))\n",
    "df['post_type'] = df['post_id'].apply(lambda x: post_types[x])\n",
    "df.rename(columns={'score': 'votes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17a20517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['post_type']=='pre'].to_csv('../data/pre_comments.tsv', sep='\\t', columns=['post_id', 'comment_id', 'parent_id', 'clean_comment', 'timestamp', 'subreddit', 'username', 'flair', 'votes', 'opp', 'win_prob', 'gametime'], index=False, escapechar=\"\\\\\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc325b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['post_type']=='post'].to_csv('../data/post_comments.tsv', sep='\\t', columns=['post_id', 'comment_id', 'parent_id', 'clean_comment','timestamp', 'subreddit', 'username', 'flair', 'votes', 'opp', 'win_prob', 'gametime'], index=False, escapechar=\"\\\\\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d5d11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['post_type']=='game'].to_csv('../data/game_comments.tsv', sep='\\t', columns=['post_id', 'comment_id', 'parent_id', 'clean_comment', 'timestamp', 'subreddit', 'username', 'flair', 'votes', 'opp', 'win_prob', 'gametime'], index=False, escapechar=\"\\\\\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04aa678b-a28b-46f0-8e84-383872bf70c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gameinfo.game_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d08c02-ec1d-436f-add8-a5b9df047aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postinfo.game_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d62460-0a1e-426a-bfcf-24b82d8e17a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1104, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postinfo[postinfo.type=='game'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901b38c-0bbd-4b05-bdae-2addee52d7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
